{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KwJZ3zMBjcRa"
   },
   "source": [
    "# Assignment\n",
    "\n",
    "## Training and analyzing Transformer-based NMT models\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jyEjvVEgXfEM"
   },
   "source": [
    "Imagine that you work for the Foreign Secretary of the United Kingdom and you have to brief the current minister for a high-profile international meeting with their German counterpart. To do so, you will need to review several policy debates at the Bundestag (the lower house of the German federal parliament). However, you do not speak German and there is no current human translator available! But don't panic: you can still develop your own custom neural machine translation (NMT) model specialised in translating German parliamentary debates into English."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UOPodpjwjtVw"
   },
   "source": [
    "In practice, you will need to complete the following tasks:\n",
    "- train an encoder-decoder Transformer model from scratch on a small dataset containing only short sentences.\n",
    "- conduct an error analysis of its translations on both shorter and longer sentences, and compare them with those of an NMT model with the same neural architecture but pre-trained on significantly more data.\n",
    "- implement some standard evaluation metrics and consider how they do (or do not) reflect the kinds of errors you observed.\n",
    "- visualize the attention weights and analyze the extent to which they can be used to identify different kinds of errors.\n",
    "- pick one of the possible strategies to improve one of the models, implement it and analyze the results in light of your observations above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hRwewpmUkGGb"
   },
   "source": [
    "### Setup\n",
    "\n",
    "First, make sure that you are connected to a GPU runtime. You should see \"T4\" next to \"Connect\" on the top right of this page. If not, click on the arrow, select \"Change runtime type\" and then \"T4 GPU\". This will ensure that training and testing run faster.\n",
    "\n",
    "Let's first install and load some relevant libraries: `evaluate` and `sacrebleu` will be useful for scoring the quality of generated translations with respect to a ground-truth reference. `bertviz` will come in handy to inspect attention weights inside the NMT model. `datasets` will allow us to easily download datasets from HuggingFace Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "VELdU8UT_26P"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q evaluate sacrebleu bertviz sacremoses accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "nPbq-_xO__DU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Disable HuggingFace progress bars to avoid xet-core conflict\n",
    "os.environ['HF_HUB_DISABLE_PROGRESS_BARS'] = '1'\n",
    "os.environ['HF_HUB_DISABLE_TELEMETRY'] = '1'\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "import evaluate\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    MarianMTModel,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    set_seed\n",
    ")\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Helper function to get the best available device\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device: {get_device()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qr2VnC5Y2EHE"
   },
   "source": [
    "We will then instantiate an encoder-decoder Transformer model based on the [configuration](https://huggingface.co/Helsinki-NLP/opus-mt-de-en/blob/main/config.json) of a popular NMT model, MarianMT. Note that after loading the `tokenizer` and `config`, we do not use `from_pretrained` for our model. This means that its architecture is identical to MarianMT but the parameters are randomly initialised.\n",
    "\n",
    "Since there is no parallel data available for the Bundestag parliamentary debates, to train and validate our model we will instead rely on the [Europarl](https://opus.nlpl.eu/Europarl/corpus/version/Europarl) dataset, a parallel corpus containing the translations of the proceedings of the European Parliament. We focus on the subset consisting of paired German and English sentences. We curate a version of this dataset with 3 data splits, which contain sentences of certain lengths. The train split (`train_data`) and i.i.d. validation split (`valid_data`) contain sentences of length < 10. In addition, the o.o.d. validation split (`gen_data`) contains sentences of length between 10 and 20 words. This will help us test if our model can also extrapolate to translating longer sequences, as parliamentary debates can be quite long! For this assignment you won't use a test set, as in this fictional scenario it would correspond to the Bundestag debates, for which there are no gold-truth English translations available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "kTXxxhczAfNx"
   },
   "outputs": [],
   "source": [
    "from transformers import MarianMTModel\n",
    "\n",
    "BASE_MODEL = \"Helsinki-NLP/opus-mt-de-en\" # we'll train our own model, but we'll use the tokenizer and config from this model!\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "config = AutoConfig.from_pretrained(BASE_MODEL)\n",
    "\n",
    "# Random initialization - NOT loading pretrained weights\n",
    "model = MarianMTModel(config)\n",
    "\n",
    "train_data = load_dataset(\"EdinburghNLP/europarl-de-en-mini\", split=\"train\") # we'll train on europarl sentences of length < 10.\n",
    "valid_data = load_dataset(\"EdinburghNLP/europarl-de-en-mini\", split=\"validation\")\n",
    "gen_data = load_dataset(\"EdinburghNLP/europarl-de-en-mini\", split=\"gen_val\") # length generalization data (length 10-20)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rjx_D9FaIHbu"
   },
   "source": [
    "Just so you can get a sense of the model architecture and a sample of the training data, let's print them both in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "cVIAO9yjHwWl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MarianMTModel(\n",
      "  (model): MarianModel(\n",
      "    (shared): Embedding(58101, 512, padding_idx=58100)\n",
      "    (encoder): MarianEncoder(\n",
      "      (embed_tokens): Embedding(58101, 512, padding_idx=58100)\n",
      "      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
      "      (layers): ModuleList(\n",
      "        (0-5): 6 x MarianEncoderLayer(\n",
      "          (self_attn): MarianAttention(\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (activation_fn): SiLU()\n",
      "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (decoder): MarianDecoder(\n",
      "      (embed_tokens): Embedding(58101, 512, padding_idx=58100)\n",
      "      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
      "      (layers): ModuleList(\n",
      "        (0-5): 6 x MarianDecoderLayer(\n",
      "          (self_attn): MarianAttention(\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (activation_fn): SiLU()\n",
      "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (encoder_attn): MarianAttention(\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=512, out_features=58101, bias=False)\n",
      ")\n",
      "{'de': '(Beifall)', 'en': '(Applause)'}\n",
      "{'de': 'Deswegen, versprühen Sie das europäische Lebensgefühl!', 'en': 'So make Europe come alive in people’s minds!'}\n",
      "{'de': 'Ich danke Ihnen nochmals für Ihren Besuch.', 'en': 'Thank you once again for your visit.'}\n",
      "{'de': 'Die Abstimmung findet um 11.30\\xa0Uhr statt.', 'en': 'The vote will take place at 11.30 a.m.'}\n",
      "{'de': '(Das Parlament genehmigt das Protokoll.)', 'en': '(The Minutes were approved)'}\n",
      "{'de': 'Frau Präsidentin! Ich habe selbstverständlich für diese Entschließung gestimmt.', 'en': 'Madam President, I voted, of course, for this resolution.'}\n",
      "{'de': 'Wie kommt man jetzt aus der Situation heraus?', 'en': 'How can the situation now be remedied?'}\n",
      "{'de': 'Das wird von unseren Bürgern mit Sicherheit nicht erwartet.', 'en': 'This is certainly not what people want.'}\n",
      "{'de': 'Ich will das gar nicht in Abrede stellen.', 'en': 'I do not dispute that.'}\n",
      "{'de': 'Woher weiß ich das?', 'en': 'How do I know that?'}\n"
     ]
    }
   ],
   "source": [
    "print(model)\n",
    "for example in train_data[:10]['translation']:\n",
    "    print(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eMh5Xboz4Fwf"
   },
   "source": [
    "Next, let's postprocess these datasets to tokenize the sentence pairs in the source (German) and target (English) languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "aPsgbn1zDa9j"
   },
   "outputs": [],
   "source": [
    "def create_preprocess_function(tokenizer):\n",
    "  # global variables are bad! the preprocess function needs the tokenizer and the examples.\n",
    "  def preprocess_function(examples):\n",
    "      \"\"\"Preprocess the data.\"\"\"\n",
    "      inputs = [ex[\"de\"] for ex in examples[\"translation\"]]\n",
    "      targets = [ex[\"en\"] for ex in examples[\"translation\"]]\n",
    "      model_inputs = tokenizer(\n",
    "          inputs, text_target=targets, return_tensors=\"pt\", padding=True\n",
    "      )\n",
    "      return model_inputs\n",
    "  return preprocess_function\n",
    "\n",
    "train_data = train_data.map(create_preprocess_function(tokenizer), batched=True)\n",
    "valid_data = valid_data.map(create_preprocess_function(tokenizer), batched=True)\n",
    "gen_data = gen_data.map(create_preprocess_function(tokenizer), batched=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mOge3fLDoSnN"
   },
   "source": [
    "### Exercise 1: Training your model (15 marks)\n",
    "\n",
    "Your first task is to execute the code below to train your NMT model on the train split of the Europarl dataset.\n",
    "\n",
    "Note that the hyper-parameters for training (e.g., batch size and learning rate) and the frequency of logging/saving/evaluating the model are provided as arguments of `Seq2SeqTrainingArguments`. The meaning of each argument is explained with in-line comments; you can find more details in the [official library documentation](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Seq2SeqTrainingArguments).\n",
    "\n",
    "During training, the `Seq2SeqTrainer` will report the losses for train, i.i.d. validation, and o.o.d. validation splits in the form of a table. Optionally, you can decide if you want to also log the metrics to the project specified by the `WANDB_PROJECT` environment variable. When prompted, follow the instructions to provide your WandB token (similar to Lab 3). To enable WandB logging, set `report_to` to `\"wandb\"`; to disable WandB, set the same argument to `[]`.\n",
    "\n",
    "A checkpoint of your model will be saved on your Colab instance at the directory specified as `output_dir` (click on the folder icon in the left sidebar to navigate there).\n",
    "\n",
    "**Important**: the models in `output_dir` are deleted every time you reload Colab. After training, make sure to compress it with the command `!zip -r my-de-en-nmt.zip my-de-en-nmt` and either dowload a copy on your laptop or move it to your Google Drive with `!cp my-de-en-nmt.zip /content/drive/MyDrive/` after [connecting your notebook to your Google Drive](https://stackoverflow.com/questions/47744131/colaboratory-can-i-access-to-my-google-drive-folder-and-file). This will allow you to reload the zip file for the subsequent exercises if you need to reload Colab for any reason. To unzip the reloaded zip file, you can use `!unzip my-de-en-nmt.zip`. Allow a few minutes for each step of this process.\n",
    "\n",
    "Now execute the code below to train your model! You can expect training to take approximately 45 minutes on a T4 GPU. Please don't close your browser or\n",
    "disconnect from your internet connection as training may stop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Pr793M2PFwWx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device: mps\n",
      "MPS detected: Using batch_size=32 with gradient_accumulation_steps=4\n",
      "Effective batch size: 128 (matching T4 configuration)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "NUM_EPOCHS = 5\n",
    "BATCH_SIZE = 32  # Reduced from 128 for MPS - much faster on Mac\n",
    "\n",
    "os.environ[\"WANDB_PROJECT\"] = \"anlp-assignment\"  # name your W&B project\n",
    "\n",
    "# Check if GPU is available (CUDA or MPS)\n",
    "use_gpu = torch.cuda.is_available() or torch.backends.mps.is_available()\n",
    "is_mps = torch.backends.mps.is_available() and not torch.cuda.is_available()\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    auto_find_batch_size=True,                # Adjust batch size automagically based on available memory\n",
    "    output_dir='my-de-en-nmt',                # Specify where the model is saved\n",
    "    save_total_limit=1,\n",
    "    predict_with_generate=False,              # Do not calculate extrinsic metrics\n",
    "    logging_steps=20,                         # Log metrics to WandB this often\n",
    "    save_steps=500,                           # Save model this often\n",
    "    eval_steps=500,                           # Evaluate model this often\n",
    "    eval_strategy=\"steps\",\n",
    "    fp16=torch.cuda.is_available(),           # Use half precision for training if on CUDA GPU\n",
    "    fp16_full_eval=torch.cuda.is_available(), # Use half precision during evaluation if on CUDA\n",
    "    group_by_length=True,                     # Group examples by length to minimise padding\n",
    "    generation_max_length=20,                 # Maximum number of tokens generated\n",
    "    dataloader_pin_memory=False,              # Disable pin_memory for MPS compatibility\n",
    "    use_cpu=(not use_gpu),                    # Use CPU if no GPU available\n",
    "    gradient_accumulation_steps=4 if is_mps else 1,  # Accumulate gradients to simulate larger batch\n",
    "    #report_to=\"wandb\",                       # Report metrics to WandB if that helps...\n",
    "    report_to=[],                             # ...or not, if you prefer\n",
    ")\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, padding=True)\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset={\"val\": valid_data, \"gen\": gen_data},\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(f\"Training on device: {get_device()}\")\n",
    "if is_mps:\n",
    "    print(f\"MPS detected: Using batch_size={BATCH_SIZE} with gradient_accumulation_steps=4\")\n",
    "    print(f\"Effective batch size: {BATCH_SIZE * 4} (matching T4 configuration)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lrd4BCtpzQro"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='261' max='8955' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 261/8955 21:17 < 11:54:34, 0.20 it/s, Epoch 0.15/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the model\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In your report**: Plot the losses for each of the 3 splits across training steps and briefly discuss their relationship and their trends. Your answer should include what you would expect to see if the model is training correctly and fully, and what the plots actually indicate.\n",
    "\n",
    "(Note: regardless of whether you think the model *could* be trained better, please use the model trained with the provided hyperparameters for the remainder of the assignment. This will make your results comparable with the other groups.)\n",
    "\n",
    "**Page limit: half a page** (Half pages are marked with lines in the report template.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Error Analysis (25 marks)\n",
    "\n",
    "The losses in the i.i.d. and o.o.d. splits give you a sense of the overall ability of your NMT model to generalise to new examples. However:\n",
    "- these losses only show the negative log probability the model assigns to the correct translation. This is not necessarily equivalent to measuring the model's ability to *generate* such translations.\n",
    "- moreover, these losses do not inform you about the types of errors present in your model's translations.\n",
    "\n",
    "Hence, you decide to conduct a more **fine-grained error analysis**, by comparing the model prediction in English with the ground-truth reference in English.\n",
    "\n",
    "In addition, you decide to compare your model's translations with those of a pretrained model. The pretrained model has the exact same architecture as yours but is trained on a mixture of texts from different genres (i.e., it is not specialised for parliamentary discussion) and on significantly more data.\n",
    "\n",
    "After some thinking, you come up with the following taxonomy of error types.\n",
    "\n",
    "- *Omission*: content present in source (and reference translation) missing in model translation.\n",
    "- *Addition/Hallucination*: content not supported by source.\n",
    "- *Grammaticality/Fluency*: ungrammatical English or awkward phrasing.\n",
    "- *Named Entities/Numbers/Punctuation*: wrong, dropped, or malformed NEs, numerals, list markers, or brackets.\n",
    "- *Incompleteness/Truncation*: translation ends prematurely or degenerates.\n",
    "- *Idioms/Formulae/Register*: failure to render fixed expressions, salutations, idioms, or formal register.\n",
    "\n",
    "**Your tasks** First, fill in the code at `### FILL IN YOUR CODE` to obtain the model `translations` from both models (the one you trained and the pre-trained one). `translations` is a list of strings (each string being the translation for an example). Then, consider the translations from each model for the first 25 examples in each validation split (i.i.d. and o.o.d.). By comparing the references and model predictions, try to annotate each example according to the error types listed above. Note that a single translation may have multiple error types. You may also find it hard to decide how to categorize certain errors (which is almost always true when annotating data). If so, take note of these cases to discuss in your report.\n",
    "\n",
    "**In your report:** Briefly discuss the results of your annotation. In particular, give 1-2 clear examples of each of the error types (from any of the model–split combinations), and also provide examples that you were not sure how to annotate. If the same type of ‘difficult’ case happened several times, would you propose adding a new error type, or just improving the annotation guidelines? Finally, using your annotated examples, plot the distribution of error types for each model-split combination and briefly discuss what you think is the reason for the most frequent kinds of errors in each of the 4 model–split combinations.\n",
    "\n",
    "**Page limit: one and a half pages**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AGrWt5gE5c-w"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "def eval(dataset, model, gen_config=None, metrics=None):\n",
    "    \"\"\"\n",
    "    dataset is one of the two splits: valid_data, gen_data\n",
    "    model is one of the two models: yours or pretrained\n",
    "    gen_config is a dictionary of arguments for model.generate()\n",
    "    metrics is a dictionary of functions to compute metrics\n",
    "    \"\"\"\n",
    "\n",
    "    results = []\n",
    "\n",
    "    if gen_config is None:\n",
    "        gen_config = {\n",
    "            \"max_length\": 20,\n",
    "            \"num_beams\": 4,\n",
    "            \"early_stopping\": True,\n",
    "        }\n",
    "\n",
    "    device = get_device()  # Use MPS if available, otherwise CUDA or CPU\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    for batch in tqdm(dataset.batch(BATCH_SIZE)):\n",
    "\n",
    "        ### FILL IN YOUR CODE\n",
    "        # Generate translations using the model\n",
    "        translations = model.generate(\n",
    "            input_ids=batch['input_ids'].to(device),\n",
    "            attention_mask=batch['attention_mask'].to(device),\n",
    "            **gen_config\n",
    "        )\n",
    "        # Decode the generated tokens to strings\n",
    "        translations = [tokenizer.decode(t, skip_special_tokens=True) for t in translations]\n",
    "\n",
    "        for i in range(len(translations)):\n",
    "            all_predictions.append(translations[i])\n",
    "            all_labels.append(batch['translation'][i]['en'])\n",
    "\n",
    "            example_metrics = {\n",
    "                    \"input\": batch[\"translation\"][i][\"de\"],\n",
    "                    \"prediction\": translations[i],\n",
    "                    \"reference\": batch[\"translation\"][i][\"en\"],\n",
    "                }\n",
    "\n",
    "            results.append(\n",
    "                example_metrics\n",
    "            )\n",
    "\n",
    "    global_metrics = {}\n",
    "    if metrics is not None:\n",
    "        for metric_name, metric_func in metrics.items():\n",
    "            global_metrics[metric_name] = metric_func(all_predictions, all_labels)\n",
    "\n",
    "    return results, global_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Cw9rVM1Alh6"
   },
   "outputs": [],
   "source": [
    "models = {\"my\": \"my-de-en-nmt\", \"pretrained\": \"Helsinki-NLP/opus-mt-de-en\"}\n",
    "datasets = {\"i.i.d.\": valid_data, \"o.o.d.\": gen_data}\n",
    "metrics = {}\n",
    "\n",
    "# For now, we use the default generation configuration and do not evaluate metrics\n",
    "# This is just to obtain the model translations\n",
    "all_results = {}\n",
    "for a, model_name in models.items():\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name, attn_implementation='eager')\n",
    "    for b, dataset in datasets.items():\n",
    "        print(f\"Evaluating {a} model on {b} split\")\n",
    "        results, global_metrics = eval(dataset, model, metrics=metrics)\n",
    "        all_results[f\"{a} {b}\"] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y0O8sUBIXu8v"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "for title, results in all_results.items():\n",
    "    print(f\"--- {title} ---\")\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    # Filter for examples with at least 5 words in the input\n",
    "    results_df_filtered = results_df[results_df['input'].str.split().str.len() >= 5]\n",
    "\n",
    "    print(\"\\nFirst 25 translations:\")\n",
    "    # Check if there are enough examples after filtering\n",
    "    if len(results_df_filtered) >= 25:\n",
    "        for index, row in results_df_filtered.head(25).iterrows():\n",
    "            print(f\"Input: {row['input']}\")\n",
    "            print(f\"Prediction: {row['prediction']}\")\n",
    "            print(f\"Reference: {row['reference']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D23OtP3_t7wp"
   },
   "source": [
    "### Exercise 3: Evaluation Metrics (15 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O-qNmlbuLvtr"
   },
   "source": [
    "After having inspected the translations and performed an error analysis, you should develop an appropriate set of extrinsic metrics for your models' generation. Specifically, you decide to focus on the following metrics:\n",
    "\n",
    "- *BLEU*: A machine translation metric based on modified n-gram (typically 1–4) precision against one or more references, combined via a geometric mean and a brevity penalty. It rewards exact n-gram overlap, which can under-value valid paraphrases; smoothing is often used for short sentences.\n",
    "\n",
    "- *ChrF*: A character n-gram F-score that combines precision and recall of character n-grams (commonly with β=2, i.e., chrF2). It is language- and tokenization-agnostic, capturing morphology and minor spelling variants better than word-level metrics.\n",
    "\n",
    "- *Length ratio* (|y|/|x| by whitespace tokens): The ratio of the number of whitespace-delimited tokens in the output y to the source x. It’s a simple diagnostic for verbosity/compression (values >1 mean longer outputs) but can be misleading across languages or with different tokenization schemes.\n",
    "\n",
    "- *Bigram repetition rate*: The proportion of bigrams in a generated text that repeat, i.e., 1 − (# unique bigrams / # total bigrams). For text with less than 3 tokens, this metric should return a value of 0. Higher values signal degenerative looping or redundancy; it flags repetition but does not measure semantic quality.\n",
    "\n",
    "Each of these metrics is computed at the dataset level.\n",
    "\n",
    "**In the code below**: implement the 4 metrics described above. For BLEU and ChrF, you should use the default implementations and hyperparameters provided by [`evaluate`](https://huggingface.co/evaluate-metric/spaces). For Length ratio and Bigram repetition rate, you should implement them with `numpy`.\n",
    "\n",
    "**In your report**: First, compute the value of these metrics for both models and both datasets. Report their values, then briefly discuss them. You may want to include the following points in your discussion:\n",
    "- Which metrics show similar trends and why?\n",
    "- Which of the error types in the Exercise 2 taxonomy can each metric capture?\n",
    "\n",
    "If you find anything else of note, feel free to include it in your answer.\n",
    "\n",
    "**Page limit: one page**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "83N4IT-x52BY"
   },
   "outputs": [],
   "source": [
    "def bleu_func(all_predictions, all_labels):\n",
    "    \"\"\"Computes the BLEU score, a metric based on modified n-gram precision.\"\"\"\n",
    "    bleu_score = evaluate.load(\"bleu\")\n",
    "    decoded_preds = tokenizer.batch_decode(all_predictions, skip_special_tokens=True)\n",
    "    bleu_result = bleu_score.compute(predictions=decoded_preds, references=[[label] for label in all_labels])\n",
    "    return bleu_result['bleu']\n",
    "\n",
    "\n",
    "def chrf_func(all_predictions, all_labels):\n",
    "    \"\"\"Computes the chrF score, a character n-gram F-score metric.\"\"\"\n",
    "    chrf_score = evaluate.load(\"chrf\")\n",
    "    decoded_preds = tokenizer.batch_decode(all_predictions, skip_special_tokens=True)\n",
    "    chrf_result = chrf_score.compute(predictions=decoded_preds, references=all_labels)\n",
    "    return chrf_result['chrf']\n",
    "\n",
    "\n",
    "def lenr_func(all_predictions, all_labels):\n",
    "    \"\"\"Computes the Length Ratio (LenR) metric.\"\"\"\n",
    "    decoded_preds = tokenizer.batch_decode(all_predictions, skip_special_tokens=True)\n",
    "    total_len_ratio = 0.0\n",
    "    for pred, label in zip(decoded_preds, all_labels):\n",
    "        pred_len = len(pred.split())\n",
    "        label_len = len(label.split())\n",
    "        if label_len > 0:\n",
    "            total_len_ratio += pred_len / label_len\n",
    "    average_len_ratio = total_len_ratio / len(all_labels)\n",
    "    return average_len_ratio\n",
    "\n",
    "\n",
    "def repr_func(all_predictions, all_labels):\n",
    "    \"\"\"Computes the Repetition Rate (Repr) metric.\"\"\"\n",
    "    decoded_preds = tokenizer.batch_decode(all_predictions, skip_special_tokens=True)\n",
    "    total_repr = 0.0\n",
    "    for pred in decoded_preds:\n",
    "        words = pred.split()\n",
    "        if len(words) == 0:\n",
    "            continue\n",
    "        unique_words = set(words)\n",
    "        total_repr += 1 - (len(unique_words) / len(words))\n",
    "    average_repr = total_repr / len(decoded_preds)\n",
    "    return average_repr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Rx_iyJhb1aQ"
   },
   "outputs": [],
   "source": [
    "models = {\"my\": \"my-de-en-nmt\", \"pretrained\": \"Helsinki-NLP/opus-mt-de-en\"}\n",
    "datasets = {\"i.i.d.\": valid_data, \"o.o.d.\": gen_data}\n",
    "metrics = {\"bleu\": bleu_func, \"chrf\": chrf_func, \"lenr\": lenr_func, \"repr\": repr_func}\n",
    "\n",
    "all_results = {}\n",
    "for a, model_name in models.items():\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name, attn_implementation='eager')\n",
    "    for b, dataset in datasets.items():\n",
    "        print(f\"Evaluating {a} model on {b} split:\")\n",
    "        results, global_metrics = eval(dataset, model, metrics=metrics)\n",
    "        print(global_metrics)\n",
    "        all_results[f\"{a} {b}\"] = results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A7yJWRsfOB_U"
   },
   "source": [
    "### Exercise 4: Attention coverage (25 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wlhfUJnTlnae"
   },
   "source": [
    "You've seen that sometimes the translation model doesn't include all the information from the source sequence in its predicted translation, a phenomenon known as *under-translation*. It's been claimed that a symptom of under-translation is the lack of \"attention coverage\". The idea behind attention coverage is that to correctly translate a source sentence, a model should attend strongly on each of its tokens at least once. This exercise explores that claim.\n",
    "\n",
    "**In the code below**: implement an estimator of attention coverage, `attention_coverage_stats`. This function takes the model, tokenizer, source sentences, predicted sentences, a threshold for low coverage (0.2 in this case), and a batch size as input. For each example, it should extract the cross-attention weights from the last layer of the decoder. It should then calculate the coverage for each source sentence token by taking the maximum attention weight over all heads and target sentence positions. After excluding padded tokens, it should calculate the fraction of source tokens with coverage below the specified threshold. It should then create `low_frac_list`, a list containing such a fraction for each example.\n",
    "\n",
    "Once you've done that, the provided code will plot the distribution of attention coverage for each model-split combination.\n",
    "\n",
    "\n",
    "**In your report**: Discuss the relationship of attention coverage with errors types and metrics that indicate under-translation. Overall, do you find support for the claim that attention coverage is useful for identifying instances of under-translation when a reference is not available? Your answer should include plots of the distribution of attention coverage (you can use our code for this, or modify it if you wish), and may also include specific examples (e.g. visualizations with `bertviz` introduced in Lab 4), or results from earlier questions. Do not assume the marker will see (or has seen) your answers to previous questions; make this answer self-contained.\n",
    "\n",
    "**Page limit: one and a half pages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GgiR4fok7i2z"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = get_device()  # Use MPS if available, otherwise CUDA or CPU\n",
    "\n",
    "@torch.no_grad()\n",
    "def attention_coverage_stats(model, tokenizer, sources, targets, threshold=0.2, batch_size=BATCH_SIZE):\n",
    "    low_frac_list = []\n",
    "\n",
    "    # extract cross-attention weights from last layer of decoder\n",
    "    for i in range(0, len(sources), batch_size):\n",
    "        batch_sources = sources[i:i+batch_size]\n",
    "        batch_targets = targets[i:i+batch_size]\n",
    "\n",
    "        # Tokenize inputs and targets\n",
    "        inputs = tokenizer(batch_sources, return_tensors=\"pt\", padding=True).to(device)\n",
    "        with tokenizer.as_target_tokenizer():\n",
    "            targets_tokenized = tokenizer(batch_targets, return_tensors=\"pt\", padding=True).to(device)\n",
    "\n",
    "        # Forward pass with attention outputs\n",
    "        outputs = model(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            decoder_input_ids=targets_tokenized[\"input_ids\"],\n",
    "            decoder_attention_mask=targets_tokenized[\"attention_mask\"],\n",
    "            output_attentions=True,\n",
    "        )\n",
    "\n",
    "        # Get cross-attention weights from the last decoder layer\n",
    "        cross_attentions = outputs.decoder_attentions[-1]  # shape: (batch_size, num_heads, target_len, source_len)\n",
    "\n",
    "        # Average over heads\n",
    "        cross_attentions_avg = cross_attentions.mean(dim=1)  # shape: (batch_size, target_len, source_len)\n",
    "\n",
    "        for j in range(cross_attentions_avg.size(0)):\n",
    "            attn_weights = cross_attentions_avg[j]  # shape: (target_len, source_len)\n",
    "            source_coverage = attn_weights.sum(dim=0)  # shape: (source_len,)\n",
    "\n",
    "            low_coverage_tokens = (source_coverage < threshold).sum().item()\n",
    "            total_source_tokens = source_coverage.size(0)\n",
    "            low_frac = low_coverage_tokens / total_source_tokens\n",
    "            low_frac_list.append(low_frac)\n",
    "\n",
    "    return np.array(low_frac_list)\n",
    "\n",
    "models = {\"my\": \"my-de-en-nmt\", \"pretrained\": \"Helsinki-NLP/opus-mt-de-en\"}\n",
    "datasets = {\"i.i.d.\": valid_data, \"o.o.d.\": gen_data}\n",
    "metrics = {\"bleu\": bleu_func, \"chrf\": chrf_func, \"lenr\": lenr_func, \"repr\": repr_func}\n",
    "\n",
    "for model_name, model_path in models.items():\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_path, attn_implementation='eager').to(device)\n",
    "    for split, _ in datasets.items():\n",
    "        sources = [example['input'] for example in all_results[f\"{model_name} {split}\"]]\n",
    "        predictions = [example['prediction'] for example in all_results[f\"{model_name} {split}\"]]\n",
    "        low_frac = attention_coverage_stats(model, tokenizer, sources, predictions, threshold=0.2)\n",
    "\n",
    "        print(f\"Coverage ({model_name} {split}): {low_frac.mean():.3f}\")\n",
    "\n",
    "        plt.figure(figsize=(6,4))\n",
    "        plt.hist(low_frac, bins=20, alpha=0.6, label=split)\n",
    "        plt.title(f\"Low-coverage source token fraction (threshold=0.2) - {model_name}\")\n",
    "        plt.xlabel(\"Low-coverage fraction per sentence\")\n",
    "        plt.ylabel(\"Count\")\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Z3jAzd7b2L9"
   },
   "source": [
    "### Exercise 5: Improving your model (20 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vo6lVJGuCQO4"
   },
   "source": [
    "Now that you have completed a thorough evaluation, you would like to improve your model's translation quality, including generalizing to longer sentence lengths and avoiding degenerate decoding behaviors. There are many ways you could potentially do this, but for this question we would like you to choose **exactly one** of the following options:\n",
    "\n",
    "1) Explore different sampling strategies for decoding.\n",
    "\n",
    "OR\n",
    "\n",
    "2) Explore relative position encoding instead of abolute position encoding.\n",
    "\n",
    "Each option is described in slightly more detail below.\n",
    "\n",
    "**In the report**:\n",
    "Briefly report on your experiments. Which option did you choose, and what did you expect to find? That is, which aspects of model performance did you expect to improve, and why? What did you actually find? Were there any downsides to you changes? *Note*: You do not need to demonstrate an actual improvement in order to do well on this question; rather you need to demonstrate careful thinking and experiments.\n",
    "\n",
    "**Page limit: one page**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qQnoYPv7PKHo"
   },
   "source": [
    "**Warning:** If you have struggled with the coding in the previous questions, or have spent a lot of time already on the assignment, we encourage you to pursue Option 1. Option 2 is more challenging to implement in Huggingface `transformers`, so a strong answer to it can receive more marks than a strong answer to Option 1. However, simply implementing a solution to Option 2 is not sufficient to do well on this question: the mark is largely based on your discussion and analysis. Therefore, a strong answer to Option 1 will still do better than a weak answer to Option 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "olhbD1aDkWQl"
   },
   "source": [
    "### Option 1\n",
    "\n",
    "Explore different sampling strategies to determine if these benefit the translation quality. These should include, at the very least:\n",
    "\n",
    "- *Beam size*: In beam search decoding for encoder–decoder models, beam size is the number of partial hypotheses kept at each step. Larger beams explore more options and can improve quality up to a point, but cost grows roughly linearly and they can sometimes reduce diversity or favor safe/short answers; beam size 1 is greedy decoding.\n",
    "\n",
    "- *Length penalty*: Because sequence probabilities multiply token probabilities, raw likelihood favors short outputs; a length penalty rescales scores to counter this bias. The final beam scores are normalised as score = log P(y) / (len(y) ** length_penalty). Values > 1.0 encourage longer outputs; < 1.0 favor shorter ones.\n",
    "\n",
    "*Hint:* You can implement these by passing in hyperparameters using the `gen_config` argument during evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "njq2RQmkP0un"
   },
   "source": [
    "### Option 2\n",
    "\n",
    "Switch from the original sinusoidal embeddings of Marian MT (absolute position encoding) to relative position encoding, as it may generalise better to longer sequence lengths. Specifically, use *strictly* one of T5-style learned relative position encoding, ALiBi, or RoPE.\n",
    "\n",
    "*Hint*: There are various ways to implement this, some easier than others. You may use/adapt code from existing model implementations, but if you do, please note it in your report. You will need to re-train your model in order to get your modification to work."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
